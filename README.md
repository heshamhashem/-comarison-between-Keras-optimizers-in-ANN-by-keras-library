# -comarison-between-Keras-optimizers-in-ANN-by-keras-library
comarison between Keras optimizers in ANN by keras library

1-class SGD: Gradient descent (with momentum) optimizer.

2-class Adagrad: Optimizer that implements the Adagrad algorithm.

3-class Adadelta: Optimizer that implements the Adadelta algorithm.Â¶

4-class RMSprop: Optimizer that implements the RMSprop algorithm. 

5-class Adam: Optimizer that implements the Adam algorithm.

6-class Adamax: Optimizer that implements the Adamax algorithm.

7-class Nadam: Optimizer that implements the NAdam algorithm. 

8-class AMSgrad: is a recent proposed improvement to Adam.
